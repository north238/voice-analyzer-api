# リアルタイム翻訳システム - 実装計画

## 全体の方針

既存の音声解析API（Whisper + opus-mt）を基盤として、段階的にリアルタイム処理機能を追加していく。各Phaseで動作するシステムを作り、確実に学習を積み重ねる。

---

## Phase 1: 疑似リアルタイム処理（チャンクベース）

### 目的
リアルタイム処理の基本的な流れを理解する。既存のHTTP APIを拡張し、チャンク単位での処理を実装する。

### 実装内容

#### 1.1 音声チャンク処理の実装
- **新規エンドポイント**: `POST /translate-chunk`
- **入力**: 音声チャンク（3秒分）+ チャンクID + セッションID
- **処理フロー**:
  ```
  音声チャンク受信 → Whisper文字起こし → 
  opus-mt翻訳 → 結果返却
  ```
- **出力**: 文字起こしテキスト + 翻訳結果 + チャンクID

#### 1.2 セッション管理の追加
- セッションIDで複数チャンクを紐付け
- 前のチャンクの翻訳結果を保持（将来の文脈引き継ぎ用）
- メモリベースでOK（Redisは不要）

#### 1.3 クライアント側のシミュレーション
- 既存の音声ファイルを3秒ごとに分割
- 順次サーバーにPOSTして翻訳結果を表示
- Pythonスクリプトで実装（Webフロントは不要）

### 学べること
- チャンク分割の考え方
- 順次処理の流れ
- レイテンシの測定方法

### 成果物
- `/translate-chunk` エンドポイント
- チャンク分割スクリプト
- 処理時間計測機能

### 所要時間目安
2-3日

---

## Phase 2: WebSocketによるストリーミング処理

### 目的
真のリアルタイム処理を実現する。WebSocketで音声データを連続的に送受信し、非同期処理を学ぶ。

### 実装内容

#### 2.1 WebSocketエンドポイントの追加
- **新規エンドポイント**: `WS /stream-translate`
- FastAPIのWebSocket機能を使用
- 接続確立 → 音声ストリーム受信 → 結果ストリーム送信

#### 2.2 非同期処理の実装
- asyncioを使った並行処理
- 音声受信タスクと処理タスクを分離
- キューを使ったチャンク管理

#### 2.3 簡易的なクライアント実装
- WebSocketクライアント（Python）
- マイクからの音声入力（pyaudio）
- リアルタイムで翻訳結果を表示

### 学べること
- WebSocketの仕組み
- 非同期プログラミングの基礎
- 双方向通信の実装

### 成果物
- `/stream-translate` WebSocketエンドポイント
- WebSocketクライアントスクリプト
- 接続管理とエラーハンドリング

### 所要時間目安
4-5日

---

## Phase 3: 実用性向上

### 目的
リアルタイムシステムの課題（文の途中で切れる、処理が追いつかない）に対処し、実用的なシステムに近づける。

### 実装内容

#### 3.1 VAD（音声区間検出）の導入
- sileroVADまたはwebrtcvadを使用
- 無音区間で音声を区切る
- 文の途中で切れる問題を軽減

#### 3.2 文脈の引き継ぎ
- 前のチャンクの翻訳結果を次の処理に渡す
- opus-mtへの入力を工夫（プロンプトエンジニアリング）
- 翻訳精度の向上を測定

#### 3.3 レイテンシ最適化
- 処理時間のボトルネック特定
- モデルサイズの調整（Whisper: small → base）
- バッファサイズの調整

#### 3.4 エラーハンドリング強化
- 接続切断時の処理
- 音声が途切れた時の処理
- 処理が追いつかない時のスキップロジック

### 学べること
- VADの仕組みと重要性
- 文脈を保持する方法
- パフォーマンスチューニング
- エッジケースへの対処

### 成果物
- VAD統合版システム
- 文脈引き継ぎ機能
- パフォーマンス測定レポート
- エラーハンドリング実装

### 所要時間目安
5-7日

---

## 技術スタック

### 既存（継承）
- FastAPI
- faster-whisper (small)
- transformers (opus-mt)
- Docker

### 新規追加
- WebSocket (FastAPI標準)
- asyncio (Python標準)
- pyaudio（クライアント側の音声入力）
- sileroVAD or webrtcvad（Phase 3）

---

## ファイル構成（予定）

```
app/
├── main.py                          # 既存APIとWebSocketエンドポイント
├── config.py
├── services/
│   ├── audio_processor.py           # 既存
│   ├── translator.py                # 既存
│   ├── streaming_processor.py       # 新規：ストリーミング処理
│   ├── session_manager.py           # 新規：セッション管理
│   └── vad_detector.py              # 新規（Phase 3）：VAD
├── utils/
│   ├── normalizer.py                # 既存
│   └── performance_monitor.py       # 新規：レイテンシ測定
└── websocket/
    ├── handler.py                   # 新規：WebSocket処理
    └── protocol.py                  # 新規：メッセージフォーマット

client/
├── chunk_client.py                  # Phase 1：チャンクベースクライアント
├── websocket_client.py              # Phase 2：WebSocketクライアント
└── audio_input.py                   # 音声入力処理

tests/
├── test_chunk_processing.py
├── test_websocket.py
└── test_vad.py
```

---

## 各Phaseの検証方法

### Phase 1
- 既存の音声ファイルを3秒ごとに分割して送信
- 各チャンクの処理時間を測定
- 翻訳結果の正確性を目視確認

### Phase 2
- マイクから実際に音声入力
- WebSocket接続の安定性を確認
- 処理の遅延時間を測定

### Phase 3
- VADあり/なしで翻訳精度を比較
- 長時間の連続使用でメモリリークがないか確認
- 意図的に接続を切断してエラーハンドリングを検証

---

## 成功基準

### Phase 1
- 3秒チャンクの処理が3秒以内に完了する
- 連続10チャンクを正常に処理できる

### Phase 2
- WebSocket接続が30秒以上安定して維持できる
- 音声入力から翻訳結果表示までのレイテンシが5秒以内

### Phase 3
- VAD導入により文の区切りが自然になる
- 10分間の連続処理でメモリ使用量が安定
- 接続切断から再接続まで自動復旧できる

---

## リスクと対策

### リスク1: Raspberry Piの処理能力不足
- **対策**: Whisperモデルをtiny/baseに切り替え
- **対策**: クライアント側で音声を圧縮

### リスク2: WebSocket実装の複雑さ
- **対策**: Phase 1で基礎を固めてから移行
- **対策**: 既存のWebSocketライブラリ・サンプルコードを参考

### リスク3: VADの精度問題
- **対策**: 複数のVADライブラリを試して比較
- **対策**: VADなしでも動作するフォールバック実装

---

## 次のステップ

1. Phase 1の詳細設計（エンドポイント仕様）
2. 既存コードの確認と改修箇所の特定
3. 開発環境のセットアップ
4. Phase 1の実装開始
