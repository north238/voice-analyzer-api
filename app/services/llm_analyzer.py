import json
import requests
import time
import re

from utils.logger import logger

OLLAMA_URL = "http://local-llm:11434/api/chat"
# MODEL_NAME = "qwen2.5:3b-instruct-q8_0" è»½é‡ã ãŒç²¾åº¦ä½ã„
# MODEL_NAME = "qwen2.5:7b-instruct-q4_0" ä¸­å›½èªã«å¤‰æ›ã•ã‚Œã‚‹å•é¡Œã‚ã‚Š
MODEL_NAME = "gemma2:2b-instruct-q8_0"

# SYSTEM_PROMPT = """You are a Japanese Hiragana-to-Kanji converter.
# Convert the input Hiragana text into natural Japanese with appropriate Kanji.

# Rules:
# 1. Output ONLY valid JSON
# 2. Use exact key names: "text" and "confidence"
# 3. No explanations or markdown
# 4. Maintain the original meaning and context

# Output format:
# {"text": "æ¼¢å­—æ··ã˜ã‚Šã®æ–‡ç« ", "confidence": 0.9}"""

SYSTEM_PROMPT = """ã‚ãªãŸã¯æ—¥æœ¬èªã®ã²ã‚‰ãŒãªã‚’æ¼¢å­—ã‹ãªæ··ã˜ã‚Šæ–‡ã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
å…¥åŠ›ã•ã‚ŒãŸã²ã‚‰ãŒãªã®ã¿ã®æ–‡ç« ã‚’ã€è‡ªç„¶ãªæ—¥æœ¬èªï¼ˆæ¼¢å­—ã¨ã²ã‚‰ãŒãªãŒæ··ã–ã£ãŸæ–‡ï¼‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ã€‘
1. å…¥åŠ›ã•ã‚ŒãŸã™ã¹ã¦ã®æ–‡å­—ã‚’å¤‰æ›ã™ã‚‹ã“ã¨ï¼ˆçœç•¥ãƒ»è¦ç´„ã¯ç¦æ­¢ï¼‰
2. æ–‡ã®é•·ã•ã‚’å¤‰ãˆãªã„ã“ã¨
3. å˜èªã‚’è¿½åŠ ãƒ»å‰Šé™¤ã—ãªã„ã“ã¨
4. æ—¥æœ¬èªã®ã¿ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ï¼ˆè‹±èªãƒ»ä¸­å›½èªã¯ä½¿ç”¨ç¦æ­¢ï¼‰
5. JSONå½¢å¼ã®ã¿ã§å‡ºåŠ›ã™ã‚‹ã“ã¨

ã€å¤‰æ›ä¾‹ã€‘
å…¥åŠ›: ãã‚‡ã†ã¯ã¦ã‚“ããŒã„ã„ã§ã™
å‡ºåŠ›: {"text": "ä»Šæ—¥ã¯å¤©æ°—ãŒè‰¯ã„ã§ã™", "confidence": 0.95}

å…¥åŠ›: ã‚€ã¦ã‚“ã‹ã®ã›ã£ã‘ã‚“
å‡ºåŠ›: {"text": "ç„¡æ·»åŠ ã®çŸ³é¹¸", "confidence": 0.95}

å…¥åŠ›: ã§ã‚“ã‚ã°ã‚“ã”ã†ã¯ãœã‚ã„ã¡ã«ã„
å‡ºåŠ›: {"text": "é›»è©±ç•ªå·ã¯012", "confidence": 0.9}

ã€å‡ºåŠ›å½¢å¼ã€‘
{"text": "å¤‰æ›å¾Œã®æ—¥æœ¬èª", "confidence": 0.0ã‹ã‚‰1.0ã®æ•°å€¤}

é‡è¦: å…¥åŠ›æ–‡ã‚’è¦ç´„ã›ãšã€å…¨ã¦ã®æ–‡å­—ã‚’å¤‰æ›ã—ã¦ãã ã•ã„ã€‚JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

def call_llm(text: str) -> dict:
    try:
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "user", "content": f"{SYSTEM_PROMPT}\n\nå…¥åŠ›: {text}\nå‡ºåŠ›:"}
            ],
            "options": {
                "temperature": 0.3,  # Gemma2ã¯å°‘ã—é«˜ã‚ãŒè‰¯ã„
                "top_k": 10,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "num_predict": 256,
            },
            "stream": False,
        }

        response = requests.post(OLLAMA_URL, json=payload, timeout=120)
        response.raise_for_status()
        data = response.json()

        content = data.get("message", {}).get("content", "{}")
        logger.debug(f"Raw LLM response: {content}")

        # JSONã‚’æŠ½å‡º
        clean_content = re.sub(r"```(?:json)?\s*|\s*```", "", content).strip()
        json_match = re.search(r'\{[^{}]*"text"[^{}]*\}', clean_content)

        if json_match:
            clean_content = json_match.group(0)

        parsed = json.loads(clean_content)
        return {
            "text": parsed.get("text", text),
            "confidence": float(parsed.get("confidence", 0.0)),
        }

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Network error: {e}")
        return {"text": text, "confidence": 0.0, "error": str(e)}
    except (json.JSONDecodeError, ValueError, KeyError) as e:
        logger.error(f"âŒ Parse error: {e}")
        logger.error(f"   Content: {content}")
        return {"text": text, "confidence": 0.0, "error": str(e)}
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}", exc_info=True)
        return {"text": text, "confidence": 0.0, "error": str(e)}


def smart_split(text: str, max_size: int = 50) -> list[str]:

    if len(text) <= max_size:
        return [text]

    # åŠ©è©ãƒ»æ¥ç¶šè©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ (å¾Œã‚ã§åˆ†å‰²å¯èƒ½ãªä½ç½®)
    split_pattern = r"([ã€‚ã€]|(?<=[ã¯ãŒã‚’ã«ã¸ã§ã¨ã‹ã‚‰ã‚ˆã‚Šã¾ã§])(?=[ã-ã‚“]))"

    # åˆ†å‰²å€™è£œã‚’ä½œæˆ
    parts = re.split(split_pattern, text)

    chunks = []
    current = ""

    for part in parts:
        if not part:
            continue

        # çµåˆã—ã¦ã‚‚åˆ¶é™å†…ãªã‚‰çµåˆ
        if len(current) + len(part) <= max_size:
            current += part
        else:
            # åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆ
            if current:
                chunks.append(current)
            current = part

    # æ®‹ã‚Šã‚’è¿½åŠ 
    if current:
        chunks.append(current)

    # åˆ†å‰²ã§ããªã‹ã£ãŸå ´åˆã¯å¼·åˆ¶åˆ†å‰²
    if len(chunks) == 1 and len(chunks[0]) > max_size:
        chunks = [text[i : i + max_size] for i in range(0, len(text), max_size)]

    return chunks


def analyze_with_llm(text: str) -> dict:
    start_time = time.perf_counter()

    # Gemma2ã¯50æ–‡å­—ç¨‹åº¦ã«åˆ†å‰²ã™ã‚‹ã¨ç²¾åº¦ãŒä¸ŠãŒã‚‹
    max_chunk_size = 50

    # çŸ­ã„å ´åˆã¯åˆ†å‰²ã—ãªã„
    if len(text) <= max_chunk_size:
        logger.info(f"ğŸ“ Processing full text ({len(text)} chars)")
        result = call_llm(text)

        elapsed = time.perf_counter() - start_time
        logger.info(f"âœ… Result: {result['text']}")
        logger.info(f"ğŸ“Š Confidence: {result['confidence']:.2f}")
        logger.info(f"â±ï¸  Time: {elapsed:.2f}s")

        return {
            "normalized": text,
            "converted_text": result["text"],
            "confidence": result["confidence"],
            "chunks_processed": 1,
            "chunks_failed": 1 if "error" in result else 0,
        }

    # é•·ã„å ´åˆã¯åˆ†å‰²ã—ã¦å‡¦ç†
    chunks = smart_split(text, max_size=max_chunk_size)

    logger.info(f"ğŸ“¦ Split into {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks):
        logger.info(f"  [{i+1}] '{chunk}' ({len(chunk)} chars)")

    results = []
    confidences = []
    failed = 0

    for i, chunk in enumerate(chunks):
        logger.info(f"ğŸ”„ Processing chunk {i+1}/{len(chunks)}...")
        result = call_llm(chunk)

        if "error" in result:
            failed += 1
            results.append(chunk)
            confidences.append(0.0)
        else:
            results.append(result["text"])
            confidences.append(result["confidence"])

        logger.info(
            f"  âœ“ [{i+1}] {result['text'][:50]}... (conf: {result['confidence']:.2f})"
        )

    final_text = "".join(results)
    avg_conf = sum(confidences) / len(confidences) if confidences else 0.0
    elapsed = time.perf_counter() - start_time

    logger.info("=" * 60)
    logger.info(f"âœ… Final result: {final_text}")
    logger.info(f"ğŸ“Š Avg confidence: {avg_conf:.2f}")
    logger.info(f"â±ï¸  Total time: {elapsed:.2f}s")

    return {
        "normalized": text,
        "converted_text": final_text,
        "confidence": round(avg_conf, 2),
        "chunks_processed": len(chunks),
        "chunks_failed": failed,
    }


# ä»¥å‰å®Ÿè£…ã‚³ãƒ¼ãƒ‰ï¼ˆæ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã®ãŸã‚ä¸è¦ï¼‰
# def analyze_with_llm(text: str, max_chunk_size: int = 150) -> dict:
#     # ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã„å ´åˆã¯åˆ†å‰²ã—ãªã„
#     if len(text) <= max_chunk_size:
#         logger.info(f"ğŸ“ Processing full text ({len(text)} chars)")
#         return process_single_chunk(text, 0)

#     # é•·ã„å ´åˆã¯åˆ†å‰²ã—ã¦å‡¦ç†
#     logger.info(f"ğŸ“¦ Text too long ({len(text)} chars), splitting into chunks")
#     return process_with_chunks(text, max_chunk_size)

# def process_single_chunk(text: str, chunk_index: int) -> dict:
#     start_time = time.perf_counter()

#     try:
#         logger.info(f"ğŸ”„ Processing: {text[:50]}...")

#         payload = {
#             "model": MODEL_NAME,
#             "messages": [
#                 {"role": "user", "content": f"{SYSTEM_PROMPT}\n\nå…¥åŠ›: {text}\nå‡ºåŠ›:"}
#             ],
#             "options": {
#                 "temperature": 0.3,  # Gemma2ã¯å°‘ã—é«˜ã‚ãŒè‰¯ã„
#                 "top_k": 10,
#                 "top_p": 0.9,
#                 "repeat_penalty": 1.1,
#                 "num_predict": 256,
#             },
#             "stream": False,
#         }

#         response = requests.post(OLLAMA_URL, json=payload, timeout=120)
#         response.raise_for_status()
#         data = response.json()

#         content = data.get("message", {}).get("content", "{}")
#         logger.info(f"content: {content}")

#         # JSONãƒ‘ãƒ¼ã‚¹
#         clean_content = re.sub(r"```(?:json)?\s*|\s*```", "", content).strip()

#         # JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰Šé™¤
#         json_match = re.search(r'\{[^{}]*"text"[^{}]*\}', clean_content)
#         if json_match:
#             clean_content = json_match.group(0)

#         parsed = json.loads(clean_content)
#         converted_text = parsed.get("text", text)
#         confidence = float(parsed.get("confidence", 0.0))

#         elapsed = time.perf_counter() - start_time
#         logger.info(
#             f"âœ“ Converted: {converted_text} (conf: {confidence}, {elapsed:.2f}s)"
#         )

#         return {
#             "normalized": text,
#             "converted_text": converted_text,
#             "confidence": round(confidence, 2),
#             "chunks_processed": 1,
#             "chunks_failed": 0,
#         }

#     except requests.exceptions.RequestException as e:
#         logger.error(f"âŒ Network error: {e}")
#         return {
#             "normalized": text,
#             "converted_text": text,
#             "confidence": 0.0,
#             "error": f"Network error: {str(e)}",
#         }
#     except (json.JSONDecodeError, ValueError, KeyError) as e:
#         logger.error(f"âŒ Parse error: {e}")
#         logger.error(f"   Raw response: {content}")
#         return {
#             "normalized": text,
#             "converted_text": text,
#             "confidence": 0.0,
#             "error": f"Parse error: {str(e)}",
#         }
#     except Exception as e:
#         logger.error(f"âŒ Unexpected error: {e}", exc_info=True)
#         return {
#             "normalized": text,
#             "converted_text": text,
#             "confidence": 0.0,
#             "error": str(e),
#         }


# def process_with_chunks(text: str, chunk_size: int) -> dict:
#     """é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦å‡¦ç†"""

#     # å˜ç´”ã«æ–‡å­—æ•°ã§åˆ†å‰²(ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—)
#     chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]

#     logger.info(f"ğŸ“¦ Split into {len(chunks)} chunks")
#     for i, chunk in enumerate(chunks):
#         logger.info(f"  Chunk {i+1}: '{chunk[:40]}...' ({len(chunk)} chars)")

#     converted_segments = []
#     confidence_scores = []
#     failed_count = 0
#     start_time = time.perf_counter()

#     for i, chunk in enumerate(chunks):
#         result = process_single_chunk(chunk, i)

#         if "error" in result:
#             failed_count += 1
#             converted_segments.append(chunk)
#             confidence_scores.append(0.0)
#         else:
#             converted_segments.append(result["converted_text"])
#             confidence_scores.append(result["confidence"])

#     elapsed = time.perf_counter() - start_time
#     final_text = "".join(converted_segments)
#     avg_confidence = (
#         sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
#     )

#     logger.info(f"â±ï¸  Total time: {elapsed:.2f}s")
#     logger.info("=" * 60)
#     logger.info(f"âœ… Final result: {final_text}")
#     logger.info(f"ğŸ“Š Avg confidence: {avg_confidence:.2f}")

#     return {
#         "normalized": text,
#         "converted_text": final_text,
#         "confidence": round(avg_confidence, 2),
#         "chunks_processed": len(chunks),
#         "chunks_failed": failed_count,
#     }



# def split_with_overlap(text: str, chunk_size: int = 100, overlap: int = 20) -> list[dict]:
#     # å¥èª­ç‚¹ãŒã‚ã‚‹å ´åˆã¯å¥èª­ç‚¹ã§åˆ†å‰²
#     if re.search(r'[ã€‚ã€]', text):
#         sentences = re.split(r'([ã€‚ã€])', text)
#         chunks = []
#         current_chunk = ""

#         for i in range(0, len(sentences), 2):
#             sentence = sentences[i]
#             delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
#             segment = sentence + delimiter

#             if len(current_chunk) + len(segment) <= chunk_size:
#                 current_chunk += segment
#             else:
#                 if current_chunk:
#                     chunks.append({"text": current_chunk, "trim_start": 0})
#                 current_chunk = segment

#         if current_chunk:
#             chunks.append({"text": current_chunk, "trim_start": 0})

#         return chunks

#     # å¥èª­ç‚¹ãŒãªã„å ´åˆ: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æˆ¦ç•¥
#     chunks = []
#     i = 0

#     while i < len(text):
#         # ãƒãƒ£ãƒ³ã‚¯ã®çµ‚äº†ä½ç½®
#         end = min(i + chunk_size, len(text))
#         chunk_text = text[i:end]

#         # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ä½ç½®(ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®)
#         # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ä»¥å¤–ã¯ã€overlapåˆ†ã ã‘æˆ»ã‚‹
#         next_start = end - overlap if end < len(text) else end

#         # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã§å‰Šé™¤ã™ã¹ãå…ˆé ­æ–‡å­—æ•°(æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ä»¥å¤–)
#         trim_start = overlap if i > 0 else 0

#         chunks.append({
#             "text": chunk_text,
#             "trim_start": trim_start
#         })

#         i = next_start

#     return chunks

# def analyze_with_llm(text: str) -> dict:
#     chunk_info_list = split_with_overlap(text, chunk_size=100, overlap=20)
#     logger.info(f"ğŸ“¦ Total chunks: {len(chunk_info_list)}")
#     for i, chunk_info in enumerate(chunk_info_list):
#         logger.info(f"  Chunk {i+1}: '{chunk_info['text'][:40]}...' (trim_start={chunk_info['trim_start']})")

#     converted_segments = []
#     confidence_scores = []
#     start_time = time.perf_counter()

#     try:
#         for i, chunk in enumerate(chunk_info_list):
#             chunk = chunk_info["text"]
#             trim_start = chunk_info["trim_start"]
#             logger.info(f"ğŸ”„ Processing chunk {i+1}/{len(chunk_info_list)}: {chunk[:30]}...")

#             payload = {
#                 "model": MODEL_NAME,
#                 "messages": [
#                     {"role": "system", "content": SYSTEM_PROMPT},
#                     {"role": "user", "content": chunk}
#                 ],
#                 "format": "json",
#                 "options": {
#                     "temperature": 0.1,        # å¤šæ§˜æ€§ã‚’è¨±å®¹
#                     "top_k": 3,                # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘ã‚’é¸ã¶
#                     "repeat_penalty": 1.2,     # åŒã˜å†…å®¹ã‚„ç„¡é–¢ä¿‚ãªãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
#                     "num_predict": 256         # å‡ºåŠ›é•·ã‚’åˆ¶é™ã—ã¦æš´èµ°ã‚’é˜²ã
#                 },
#                 "stream": False,
#             }

#             # --- HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä¾‹å¤–å‡¦ç† ---
#             try:
#                 response = requests.post(OLLAMA_URL, json=payload, timeout=60)
#                 response.raise_for_status()
#                 data = response.json()
#             except requests.exceptions.RequestException as e:
#                 logger.error(f"âŒ Network error on chunk {i+1}: {e}")
#                 converted_segments.append(chunk) # å¤±æ•—æ™‚ã¯ã²ã‚‰ãŒãªã®ã¾ã¾ä¿æŒ
#                 confidence_scores.append(0.0)
#                 continue

#             # --- JSONãƒ‘ãƒ¼ã‚¹ã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®ä¾‹å¤–å‡¦ç† ---
#             content = data.get("message", {}).get("content", "{}")

#             try:
#                 # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚¿ã‚°ãŒå«ã¾ã‚Œã‚‹å ´åˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
#                 clean_content = re.sub(r"```json|```", "", content).strip()
#                 parsed = json.loads(clean_content)

#                 converted_text = parsed.get("text", chunk)
#                 confidence = float(parsed.get("confidence", 0.0))

#                 if trim_start > 0:
#                     converted_text = converted_text[trim_start:]

#                 converted_segments.append(converted_text)
#                 confidence_scores.append(confidence)

#                 logger.info(f"Chunk {1+i} converted: {converted_text}")

#             except (json.JSONDecodeError, ValueError) as e:
#                 logger.error(f"âŒ Parse error on chunk {i+1}: {e} | Content: {content}")

#                 fallback_text = str(chunk[trim_start:])
#                 converted_segments.append(fallback_text)
#                 confidence_scores.append(0.0)

#         # æœ€çµ‚çš„ãªé›†è¨ˆ
#         end_time = time.perf_counter()
#         elapsed_time = end_time - start_time

#         final_text = "".join(converted_segments)
#         avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0

#         logger.info(f"â±ï¸  Processing time: {elapsed_time:.2f}s")
#         logger.info("=" * 60)
#         logger.info(f"âœ… Final: {final_text}")
#         logger.info(f"ğŸ“Š Avg confidence: {avg_confidence:.2f}")

#         return {
#             "normalized": text,
#             "converted_text": final_text,
#             "confidence": round(avg_confidence, 2)
#         }

#     except Exception as e:
#         # äºˆæœŸã›ã¬è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼
#         logger.error(f"âŒ Critical error in analyze_with_llm: {e}")
#         return {
#             "normalized": text,
#             "converted_text": "unknown",
#             "confidence": 0.0,
#             "error": str(e)
#         }
