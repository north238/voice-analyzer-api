import json
import requests
import time
import re

from utils.logger import logger

OLLAMA_URL = "http://local-llm:11434/api/chat"
# MODEL_NAME = "qwen2.5:3b"
MODEL_NAME = "qwen2.5:3b-instruct-q8_0"

SYSTEM_PROMPT = """You are a Japanese Hiragana-to-Kanji converter.
Convert the input Hiragana text into natural Japanese with appropriate Kanji.

Rules:
1. Output ONLY valid JSON
2. Use exact key names: "text" and "confidence"
3. No explanations or markdown
4. Maintain the original meaning and context

Output format:
{"text": "æ¼¢å­—æ··ã˜ã‚Šã®æ–‡ç« ", "confidence": 0.9}"""

def split_by_sentences(text: str, max_length: int = 45) -> list[str]:
    # å¥èª­ç‚¹ãŒã‚ã‚‹å ´åˆ
    if re.search(r'[ã€‚ã€]', text):
        sentences = re.split(r'([ã€‚ã€])', text)
        chunks = []
        current_chunk = ""

        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
            segment = sentence + delimiter

            if len(current_chunk) + len(segment) <= max_length:
                current_chunk += segment
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = segment

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    # å¥èª­ç‚¹ãŒãªã„å ´åˆ: åŠ©è©ãƒ»æ¥ç¶šè©ã®å¾Œã‚ã§åˆ†å‰²
    # ã€Œã¯ã€ã€ŒãŒã€ã€Œã‚’ã€ã€Œã«ã€ã€Œã§ã€ã€Œã¨ã€ã€Œã‹ã‚‰ã€ã€Œã¾ã§ã€ãªã©ã§åˆ†å‰²
    particles = r'(ã¯|ãŒ|ã‚’|ã«|ã¸|ã§|ã¨|ã‹ã‚‰|ã¾ã§|ã‚ˆã‚Š|ã®|ã‚„|ã‹)'

    # åŠ©è©ã®å¾Œã‚ã«ãƒãƒ¼ã‚«ãƒ¼ã‚’å…¥ã‚Œã¦åˆ†å‰²ã—ã‚„ã™ãã™ã‚‹
    marked_text = re.sub(particles, r'\1|', text)
    potential_chunks = marked_text.split('|')

    chunks = []
    current_chunk = ""

    for segment in potential_chunks:
        if not segment:
            continue

        if len(current_chunk) + len(segment) <= max_length:
            current_chunk += segment
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = segment

    if current_chunk:
        chunks.append(current_chunk)

    # ãã‚Œã§ã‚‚ç©ºã®å ´åˆã¯å˜ç´”åˆ†å‰²
    if not chunks:
        chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]

    return chunks

def analyze_with_llm(text: str) -> dict:
    chunks = split_by_sentences(text, max_length=45)

    converted_segments = []
    confidence_scores = []
    start_time = time.perf_counter()

    try:
        for i, chunk in enumerate(chunks):
            logger.info(f"ğŸ”„ Processing chunk {i+1}/{len(chunks)}...")

            payload = {
                "model": MODEL_NAME,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": chunk}
                ],
                "format": "json",
                "options": {
                    "temperature": 0.1,        # å¤šæ§˜æ€§ã‚’è¨±å®¹
                    "top_k": 3,                # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘ã‚’é¸ã¶
                    "repeat_penalty": 1.2,     # åŒã˜å†…å®¹ã‚„ç„¡é–¢ä¿‚ãªãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
                    "num_predict": 256         # å‡ºåŠ›é•·ã‚’åˆ¶é™ã—ã¦æš´èµ°ã‚’é˜²ã
                },
                "stream": False,
            }

            # --- HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä¾‹å¤–å‡¦ç† ---
            try:
                response = requests.post(OLLAMA_URL, json=payload, timeout=60)
                response.raise_for_status()
                data = response.json()
            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ Network error on chunk {i+1}: {e}")
                converted_segments.append(chunk) # å¤±æ•—æ™‚ã¯ã²ã‚‰ãŒãªã®ã¾ã¾ä¿æŒ
                confidence_scores.append(0.0)
                continue

            # --- JSONãƒ‘ãƒ¼ã‚¹ã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®ä¾‹å¤–å‡¦ç† ---
            content = data.get("message", {}).get("content", "{}")

            try:
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚¿ã‚°ãŒå«ã¾ã‚Œã‚‹å ´åˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                clean_content = re.sub(r"```json|```", "", content).strip()
                parsed = json.loads(clean_content)

                converted_text = parsed.get("text", chunk)
                confidence = float(parsed.get("confidence", 0.0))

                converted_segments.append(converted_text)
                confidence_scores.append(confidence)

                logger.info(f"Chunk {1+i} converted: {converted_text}")

            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"âŒ Parse error on chunk {i+1}: {e} | Content: {content}")
                converted_segments.append(chunk)
                confidence_scores.append(0.0)

        # æœ€çµ‚çš„ãªé›†è¨ˆ
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time

        final_text = "".join(converted_segments)
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0

        logger.info(f"â±ï¸  Processing time: {elapsed_time:.2f}s")
        logger.info("=" * 60)
        logger.info(f"âœ… Final: {final_text}")
        logger.info(f"ğŸ“Š Avg confidence: {avg_confidence:.2f}")

        return {
            "normalized": text,
            "converted_text": final_text,
            "confidence": round(avg_confidence, 2)
        }

    except Exception as e:
        # äºˆæœŸã›ã¬è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼
        logger.error(f"âŒ Critical error in analyze_with_llm: {e}")
        return {
            "normalized": text,
            "converted_text": "unknown",
            "confidence": 0.0,
            "error": str(e)
        }
