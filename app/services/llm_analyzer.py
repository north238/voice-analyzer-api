import json
import requests
import time
import re

from config import settings
from utils.logger import logger

OLLAMA_URL = "http://local-llm:11434/api/chat"
MODEL_NAME = settings.OLLAMA_MODEL

# SYSTEM_PROMPT = """You are a Japanese Hiragana-to-Kanji converter.
# Convert the input Hiragana text into natural Japanese with appropriate Kanji.

# Rules:
# 1. Output ONLY valid JSON
# 2. Use exact key names: "text" and "confidence"
# 3. No explanations or markdown
# 4. Maintain the original meaning and context

# Output format:
# {"text": "æ¼¢å­—æ··ã˜ã‚Šã®æ–‡ç« ", "confidence": 0.9}"""

SYSTEM_PROMPT = """ã‚ãªãŸã¯æ—¥æœ¬èªã®ã²ã‚‰ãŒãªã‚’æ¼¢å­—ã‹ãªæ··ã˜ã‚Šæ–‡ã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
å…¥åŠ›ã•ã‚ŒãŸã²ã‚‰ãŒãªã®ã¿ã®æ–‡ç« ã‚’ã€è‡ªç„¶ãªæ—¥æœ¬èªï¼ˆæ¼¢å­—ã¨ã²ã‚‰ãŒãªãŒæ··ã–ã£ãŸæ–‡ï¼‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ã€‘
1. å…¥åŠ›ã•ã‚ŒãŸã™ã¹ã¦ã®æ–‡å­—ã‚’å¤‰æ›ã™ã‚‹ã“ã¨ï¼ˆçœç•¥ãƒ»è¦ç´„ã¯ç¦æ­¢ï¼‰
2. æ–‡ã®é•·ã•ã‚’å¤‰ãˆãªã„ã“ã¨
3. å˜èªã‚’è¿½åŠ ãƒ»å‰Šé™¤ã—ãªã„ã“ã¨
4. æ—¥æœ¬èªã®ã¿ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ï¼ˆè‹±èªãƒ»ä¸­å›½èªã¯ä½¿ç”¨ç¦æ­¢ï¼‰
5. JSONå½¢å¼ã®ã¿ã§å‡ºåŠ›ã™ã‚‹ã“ã¨

ã€å¤‰æ›ä¾‹ã€‘
å…¥åŠ›: ãã‚‡ã†ã¯ã¦ã‚“ããŒã„ã„ã§ã™
å‡ºåŠ›: {"text": "ä»Šæ—¥ã¯å¤©æ°—ãŒè‰¯ã„ã§ã™", "confidence": 0.95}

ã€å‡ºåŠ›å½¢å¼ã€‘
{"text": "å¤‰æ›å¾Œã®æ—¥æœ¬èª", "confidence": 0.0ã‹ã‚‰1.0ã®æ•°å€¤}

é‡è¦: å…¥åŠ›æ–‡ã‚’è¦ç´„ã›ãšã€å…¨ã¦ã®æ–‡å­—ã‚’å¤‰æ›ã—ã¦ãã ã•ã„ã€‚JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

def call_llm(text: str) -> dict:
    try:
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "user", "content": f"{SYSTEM_PROMPT}\n\nå…¥åŠ›: {text}\nå‡ºåŠ›:"}
            ],
            "options": {
                "temperature": settings.OLLAMA_TEMPERATURE,  # Gemma2ã¯å°‘ã—é«˜ã‚ãŒè‰¯ã„
                "top_k": settings.OLLAMA_TOP_K,
                "top_p": settings.OLLAMA_TOP_P,
                "repeat_penalty": settings.OLLAMA_REPEAT_PENALTY,
                "num_predict": settings.OLLAMA_NUM_PREDICT,
            },
            "stream": False,
        }

        response = requests.post(OLLAMA_URL, json=payload, timeout=settings.OLLAMA_TIMEOUT)
        response.raise_for_status()
        data = response.json()

        content = data.get("message", {}).get("content", "{}")
        logger.debug(f"Raw LLM response: {content}")

        # JSONã‚’æŠ½å‡º
        clean_content = re.sub(r"```(?:json)?\s*|\s*```", "", content).strip()
        json_match = re.search(r'\{[^{}]*"text"[^{}]*\}', clean_content)

        if json_match:
            clean_content = json_match.group(0)

        parsed = json.loads(clean_content)
        return {
            "text": parsed.get("text", text),
            "confidence": float(parsed.get("confidence", 0.0)),
        }

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Network error: {e}")
        return {"text": text, "confidence": 0.0, "error": str(e)}
    except (json.JSONDecodeError, ValueError, KeyError) as e:
        logger.error(f"âŒ Parse error: {e}")
        logger.error(f"   Content: {content}")
        return {"text": text, "confidence": 0.0, "error": str(e)}
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}", exc_info=True)
        return {"text": text, "confidence": 0.0, "error": str(e)}


def smart_split(text: str, max_size: int = 50) -> list[str]:

    if len(text) <= max_size:
        return [text]

    # åŠ©è©ãƒ»æ¥ç¶šè©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ (å¾Œã‚ã§åˆ†å‰²å¯èƒ½ãªä½ç½®)
    split_pattern = r"([ã€‚ã€]|(?<=[ã¯ãŒã‚’ã«ã¸ã§ã¨ã‹ã‚‰ã‚ˆã‚Šã¾ã§])(?=[ã-ã‚“]))"

    # åˆ†å‰²å€™è£œã‚’ä½œæˆ
    parts = re.split(split_pattern, text)

    chunks = []
    current = ""

    for part in parts:
        if not part:
            continue

        # çµåˆã—ã¦ã‚‚åˆ¶é™å†…ãªã‚‰çµåˆ
        if len(current) + len(part) <= max_size:
            current += part
        else:
            # åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆ
            if current:
                chunks.append(current)
            current = part

    # æ®‹ã‚Šã‚’è¿½åŠ 
    if current:
        chunks.append(current)

    # åˆ†å‰²ã§ããªã‹ã£ãŸå ´åˆã¯å¼·åˆ¶åˆ†å‰²
    if len(chunks) == 1 and len(chunks[0]) > max_size:
        chunks = [text[i : i + max_size] for i in range(0, len(text), max_size)]

    return chunks


def analyze_with_llm(text: str) -> dict:
    start_time = time.perf_counter()

    # Gemma2ã¯50æ–‡å­—ç¨‹åº¦ã«åˆ†å‰²ã™ã‚‹ã¨ç²¾åº¦ãŒä¸ŠãŒã‚‹
    max_chunk_size = settings.MAX_TEXT_LENGTH

    # çŸ­ã„å ´åˆã¯åˆ†å‰²ã—ãªã„
    if len(text) <= max_chunk_size:
        logger.info(f"ğŸ“ Processing full text ({len(text)} chars)")
        result = call_llm(text)

        elapsed = time.perf_counter() - start_time
        logger.info(f"âœ… Result: {result['text']}")
        logger.info(f"ğŸ“Š Confidence: {result['confidence']:.2f}")
        logger.info(f"â±ï¸  Time: {elapsed:.2f}s")

        return {
            "normalized": text,
            "converted_text": result["text"],
            "confidence": result["confidence"],
            "chunks_processed": 1,
            "chunks_failed": 1 if "error" in result else 0,
        }

    # é•·ã„å ´åˆã¯åˆ†å‰²ã—ã¦å‡¦ç†
    chunks = smart_split(text, max_size=max_chunk_size)

    logger.info(f"ğŸ“¦ Split into {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks):
        logger.info(f"  [{i+1}] '{chunk}' ({len(chunk)} chars)")

    results = []
    confidences = []
    failed = 0

    for i, chunk in enumerate(chunks):
        logger.info(f"ğŸ”„ Processing chunk {i+1}/{len(chunks)}...")
        result = call_llm(chunk)

        if "error" in result:
            failed += 1
            results.append(chunk)
            confidences.append(0.0)
        else:
            results.append(result["text"])
            confidences.append(result["confidence"])

        logger.info(
            f"  âœ“ [{i+1}] {result['text'][:50]}... (conf: {result['confidence']:.2f})"
        )

    final_text = "".join(results)
    avg_conf = sum(confidences) / len(confidences) if confidences else 0.0
    elapsed = time.perf_counter() - start_time

    logger.info("=" * 60)
    logger.info(f"âœ… Final result: {final_text}")
    logger.info(f"ğŸ“Š Avg confidence: {avg_conf:.2f}")
    logger.info(f"â±ï¸  Total time: {elapsed:.2f}s")

    return {
        "normalized": text,
        "converted_text": final_text,
        "confidence": round(avg_conf, 2),
        "chunks_processed": len(chunks),
        "chunks_failed": failed,
    }
