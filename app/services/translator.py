from transformers import MarianMTModel, MarianTokenizer
from config import settings
from utils.logger import logger
from typing import Optional
import re


class Translator:
    """æ—¥è‹±ç¿»è¨³ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆHelsinki-NLP/opus-mt-ja-enï¼‰"""

    def __init__(self):
        self.model: Optional[MarianMTModel] = None
        self.tokenizer: Optional[MarianTokenizer] = None
        self.model_name = settings.TRANSLATION_MODEL
        self.max_length = settings.MAX_TRANSLATION_LENGTH
        self.device = settings.TRANSLATION_DEVICE

    def _load_model(self):
        """ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰"""
        if self.model is not None and self.tokenizer is not None:
            return

        try:
            logger.info(f"ğŸ”„ ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {self.model_name}")
            self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)
            self.model = MarianMTModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"âœ… ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            logger.exception(f"âŒ ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
            raise RuntimeError(f"ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _preprocess_text(self, text: str) -> tuple[str, dict]:
        """
        ç¿»è¨³å‰ã®å‰å‡¦ç†ï¼ˆæ•°å­—ãƒ»é›»è©±ç•ªå·ã®ä¿è­·ï¼‰

        Args:
            text: å‰å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            (å‰å‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ, ç½®æ›ãƒãƒƒãƒ—)
        """
        replacements = {}
        processed_text = text

        # é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦ä¿è­·
        phone_pattern = r"\d{10,11}"
        phones = re.findall(phone_pattern, processed_text)
        for i, phone in enumerate(phones):
            placeholder = f"__PHONE_{i}__"
            replacements[placeholder] = phone
            processed_text = processed_text.replace(phone, placeholder, 1)
            logger.debug(f"é›»è©±ç•ªå·ã‚’ä¿è­·: {phone} â†’ {placeholder}")

        return processed_text, replacements

    def _postprocess_text(self, text: str, replacements: dict) -> str:
        """
        ç¿»è¨³å¾Œã®å¾Œå‡¦ç†ï¼ˆä¿è­·ã—ãŸè¦ç´ ã‚’å¾©å…ƒï¼‰

        Args:
            text: å¾Œå‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            replacements: ç½®æ›ãƒãƒƒãƒ—

        Returns:
            å¾Œå‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        processed_text = text
        for placeholder, original in replacements.items():
            processed_text = processed_text.replace(placeholder, original)
            logger.debug(f"ä¿è­·è¦ç´ ã‚’å¾©å…ƒ: {placeholder} â†’ {original}")

        return processed_text

    def translate_text(self, text: str) -> str:
        """
        æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³

        Args:
            text: ç¿»è¨³å¯¾è±¡ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ç¿»è¨³ã•ã‚ŒãŸè‹±èªãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text or not text.strip():
            logger.warning("âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸ")
            return ""

        try:
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
            self._load_model()

            # å‰å‡¦ç†ï¼ˆæ•°å­—ãƒ»é›»è©±ç•ªå·ã®ä¿è­·ï¼‰
            preprocessed_text, replacements = self._preprocess_text(text)

            # æ–‡å˜ä½ã§åˆ†å‰²ã—ã¦ç¿»è¨³ï¼ˆç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰
            sentences = self._split_into_sentences(preprocessed_text)

            if len(sentences) > 1:
                # è¤‡æ•°æ–‡ã®å ´åˆã¯1æ–‡ãšã¤ç¿»è¨³
                logger.info(f"ğŸ“ {len(sentences)}å€‹ã®æ–‡ã«åˆ†å‰²ã—ã¦ç¿»è¨³ã—ã¾ã™")
                translated = self._translate_long_text(preprocessed_text)
            else:
                # å˜ä¸€æ–‡ã®å ´åˆã¯é€šå¸¸ã®ç¿»è¨³å‡¦ç†
                logger.info(f"ğŸ”„ ç¿»è¨³é–‹å§‹: {preprocessed_text[:50]}...")
                translated = self._translate_chunk(preprocessed_text)
                logger.info(f"âœ… ç¿»è¨³å®Œäº†: {translated[:50]}...")

            # å¾Œå‡¦ç†ï¼ˆä¿è­·è¦ç´ ã®å¾©å…ƒï¼‰
            final_text = self._postprocess_text(translated, replacements)

            return final_text

        except Exception as e:
            logger.exception(f"âŒ ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            raise RuntimeError(f"ç¿»è¨³å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def _translate_chunk(self, text: str) -> str:
        """
        å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®ç¿»è¨³å‡¦ç†

        Args:
            text: ç¿»è¨³å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆmax_lengthä»¥å†…ï¼‰

        Returns:
            ç¿»è¨³çµæœ
        """
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True, max_length=512
        ).to(self.device)

        # ç¿»è¨³ç”Ÿæˆï¼ˆå“è³ªå‘ä¸Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        # Phase 4æœ€é©åŒ–: num_beams ã‚’ 6 â†’ 4 ã«å¤‰æ›´ï¼ˆå‡¦ç†æ™‚é–“30%å‰Šæ¸›ï¼‰
        translated_tokens = self.model.generate(
            **inputs,
            num_beams=4,  # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§å“è³ªå‘ä¸Šï¼ˆ6â†’4ã«å‰Šæ¸›ã€é€Ÿåº¦å‘ä¸Šï¼‰
            no_repeat_ngram_size=3,  # 3-gramã®ç¹°ã‚Šè¿”ã—ã‚’é˜²æ­¢
            repetition_penalty=1.3,  # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ç·©å’Œï¼ˆ1.5â†’1.3ï¼‰
            length_penalty=0.8,  # çŸ­ã‚ã®ç¿»è¨³ã‚’å„ªå…ˆï¼ˆ1.0â†’0.8ï¼‰
            early_stopping=True,  # æ—©æœŸçµ‚äº†ã‚’æœ‰åŠ¹åŒ–
            max_length=512,  # æœ€å¤§å‡ºåŠ›é•·
            temperature=0.7,  # å¤šæ§˜æ€§ã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã ãŒ0.7ã§å®‰å®šæ€§å‘ä¸Šï¼‰
        )

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        translated_text = self.tokenizer.decode(
            translated_tokens[0], skip_special_tokens=True
        )

        return translated_text

    def _split_into_sentences(self, text: str) -> list[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã§åˆ†å‰²

        Args:
            text: åˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            æ–‡ã®ãƒªã‚¹ãƒˆ
        """
        # å¥ç‚¹ã§åˆ†å‰²
        sentences = text.split("ã€‚")
        result = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence:
                result.append(
                    sentence + "ã€‚" if not sentence.endswith("ã€‚") else sentence
                )
        return result

    def _translate_long_text(self, text: str) -> str:
        """
        é•·æ–‡ã‚’åˆ†å‰²ã—ã¦ç¿»è¨³

        Args:
            text: é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            åˆ†å‰²ç¿»è¨³ã•ã‚ŒãŸçµæœã‚’çµåˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # æ–‡å˜ä½ã§åˆ†å‰²
        sentences = self._split_into_sentences(text)

        logger.info(f"ğŸ“¦ {len(sentences)}å€‹ã®æ–‡ã«åˆ†å‰²ã—ã¾ã—ãŸ")

        # å„æ–‡ã‚’å€‹åˆ¥ã«ç¿»è¨³
        translated_sentences = []
        for i, sentence in enumerate(sentences):
            logger.info(f"ğŸ”„ æ–‡ {i+1}/{len(sentences)} ã‚’ç¿»è¨³ä¸­: {sentence[:30]}...")
            translated = self._translate_chunk(sentence)
            translated_sentences.append(translated)
            logger.info(f"âœ… ç¿»è¨³çµæœ: {translated[:50]}...")

        # çµåˆã—ã¦è¿”ã™
        result = " ".join(translated_sentences)
        logger.info(f"âœ… å…¨æ–‡ã®ç¿»è¨³å®Œäº†")
        return result


# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_translator_instance: Optional[Translator] = None


def get_translator() -> Translator:
    """ç¿»è¨³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _translator_instance
    if _translator_instance is None:
        _translator_instance = Translator()
    return _translator_instance


def translate_text(text: str) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ã™ã‚‹ä¾¿åˆ©é–¢æ•°

    Args:
        text: ç¿»è¨³å¯¾è±¡ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        ç¿»è¨³ã•ã‚ŒãŸè‹±èªãƒ†ã‚­ã‚¹ãƒˆ
    """
    translator = get_translator()
    return translator.translate_text(text)
