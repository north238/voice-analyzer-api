# Whisper音声認識モデルの仕様

## 📋 概要

このドキュメントは、プロジェクトで使用しているWhisper音声認識モデルの仕様と制限事項をまとめたものです。今後の開発や機能拡張時の参考資料として使用してください。

最終更新: 2026-02-03

## 🎯 Whisperモデルの基本仕様

### モデルアーキテクチャ

- **開発元**: OpenAI
- **実装**: faster-whisper (CTranslate2ラッパー)
- **使用モデル**: base（プロジェクトのデフォルト）
- **対応言語**: 多言語対応（プロジェクトでは日本語を使用）

### プロジェクトの設定

```python
# app/config.py
WHISPER_MODEL_SIZE: "base"
WHISPER_DEVICE: "cpu"
WHISPER_COMPUTE_TYPE: "int8"
WHISPER_BEAM_SIZE: 3
WHISPER_BEST_OF: 3
WHISPER_VAD_ENABLED: True
```

## ⚠️ 重要な制限事項

### 1. 30秒セグメント制限（最重要）

#### 制限内容

- **Whisperモデルは30秒のオーディオセグメントをネイティブにサポート**
- これは**モデルのアーキテクチャ上の制約**であり、変更できない
- モデルのトレーニング時に30秒のセグメントを使用している

#### 30秒を超えた場合の問題

**重要**: 30秒を超える音声を単一セグメントとして処理すると、以下の問題が発生します：

1. **切り取り（Cutoff）**
   - 30秒を超える部分が切り捨てられる
   - 音声の後半部分が認識されない

2. **幻覚（Hallucination）**
   - モデルが存在しない内容を生成する
   - 音声に含まれていないテキストが出力される
   - **これが最も重要な問題点**

3. **切り詰められた文字起こし（Truncated Transcription）**
   - 文字起こし結果が途中で終わる
   - 完全な文が出力されない

4. **精度の低下**
   - 認識精度が著しく低下する
   - 誤認識が増加する

#### 推奨される使用方法

- **30秒以内のセグメント**で処理する
- 30秒を超える音声は、適切なチャンキング戦略を使用する

### 2. 長時間音声の処理方法

30秒を超える音声を処理する場合、以下の戦略を使用する必要があります：

#### 戦略1: スライディングウィンドウ方式

```
音声: [============================================] (60秒)

処理: [========30秒========]
            [========30秒========]
                  [========30秒========]

結果を結合: "文字起こし結果1" + "文字起こし結果2" + "文字起こし結果3"
```

**特徴**:
- 30秒のウィンドウをスライドさせながら処理
- セグメント間でオーバーラップを持たせる
- 推奨オーバーラップ: `stride = chunk_length / 6` (約5秒)
- セグメントの境界で単語が切れないように調整

#### 戦略2: VADベースのバッチング

```
音声: [音声区間1][無音][音声区間2][無音][音声区間3]

VAD検出: 音声区間を検出
         ↓
バッチング: 30秒以下になるように集約
           ↓
処理: [音声区間1+2] [音声区間3] (それぞれ30秒以下)
```

**特徴**:
- 音声区間検出（VAD）を使用
- 無音部分で自然に分割
- 音声区間の前後に100msの無音を追加
- より自然なセグメント分割

#### 戦略3: チャンキング+結合

```
音声: [============================================] (90秒)

分割: [0-30秒][30-60秒][60-90秒]

処理: 各チャンクを独立して処理

結合: 結果を時系列順に結合
```

**特徴**:
- シンプルな実装
- 各チャンクは独立して処理
- セグメント境界で精度が低下する可能性あり

### 3. faster-whisperの長時間音声サポート

faster-whisperライブラリは、長時間音声を自動的にチャンキングして処理する機能を持っています：

```python
# faster-whisperは自動的にチャンキング
segments, info = whisper_model.transcribe(
    audio_path,
    language="ja",
    beam_size=3,
    vad_filter=True,  # VADを有効化
)

# セグメントは自動的に30秒以下に分割される
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
```

**自動処理内容**:
- 30秒を超える音声を自動的に分割
- VADを使用して自然な区切りで分割
- セグメント結果を時系列順に返す
- **通常のファイル処理では自動的に行われる**

## 🔧 プロジェクトにおける実装

### 現在の実装: 累積バッファ方式

```python
# app/services/cumulative_buffer.py
class CumulativeBufferConfig:
    max_audio_duration_seconds: float = 30.0  # Whisperの1セグメント上限
```

**動作**:
1. 音声チャンクを累積してバッファに保存
2. 30秒を超えると**古いチャンクを削除**
3. 3チャンクごとに累積音声全体を再文字起こし
4. 確定テキストと暫定テキストを区別

**問題点**:
- 30秒を超えると古い音声データが失われる
- 文脈が失われ、精度が低下する
- 実際の録音時間が正しく表示されない

### 推奨される改善策

#### 改善策1: 実際の経過時間の追跡

- 音声バッファは30秒に制限
- セッション開始からの実際の経過時間を別途記録
- クライアントに正確な録音時間を表示

#### 改善策2: 確定テキストの文脈保持

- 削除された音声の確定テキストは保持
- `initial_prompt`パラメータで文脈をWhisperに渡す
- 精度の低下を最小限に抑える

#### 改善策3: バッファサイズの拡張

- 30秒 → 45秒または60秒に拡張
- オーバーラップを持たせて文脈を保持
- メモリ使用量とのトレードオフ

## 📚 参考資料

### 公式ドキュメント・ディスカッション

- [Reason for 30s audio length · openai/whisper · Discussion #1118](https://github.com/openai/whisper/discussions/1118)
  - 30秒制限の技術的背景について

- [Whisper Long-Form Transcription | Medium](https://medium.com/@yoad/whisper-long-form-transcription-1924c94a9b86)
  - 長時間音声の処理方法について

- [Optimal long audio chunking when using pipeline](https://huggingface.co/openai/whisper-large-v2/discussions/67)
  - 最適なチャンキング戦略について

### Azure/OpenAI API の制限

- [Is it possible to transcribe recordings longer than 30 seconds with Azure Whisper?](https://learn.microsoft.com/en-us/answers/questions/1603769/is-it-possible-to-transcribe-recordings-longer-tha)
  - Azure Whisper API: 30秒制限 + 25MBファイルサイズ制限

### faster-whisper実装

- [5 Ways to Speed Up Whisper Transcription](https://modal.com/blog/faster-transcription)
  - faster-whisperの最適化手法

## 🎓 ベストプラクティス

### リアルタイム処理の場合

1. **累積バッファは30秒に制限**
   - Whisperの仕様に合わせる
   - メモリ効率を維持

2. **確定テキストは永続化**
   - 音声データは削除しても、テキストは保持
   - 文脈として`initial_prompt`に渡す

3. **実際の経過時間を追跡**
   - バッファ内の音声長とは別に記録
   - ユーザーに正確な情報を提供

### バッチ処理の場合

1. **faster-whisperの自動チャンキング機能を使用**
   - 手動でのチャンキング不要
   - VADで自然な区切りを検出

2. **VADを有効化**
   - `vad_filter=True`を設定
   - 無音区間で自然に分割

3. **セグメント結果を結合**
   - 時系列順に結合
   - タイムスタンプを保持

## ⚡ パフォーマンスの考慮事項

### モデルサイズと処理速度

| モデル | 精度 | 処理速度 | メモリ使用量 |
|--------|------|----------|--------------|
| tiny   | 低   | 最速     | 最小         |
| base   | 中   | 高速     | 小           |
| small  | 高   | 中速     | 中           |
| medium | 高   | 低速     | 大           |
| large  | 最高 | 最低速   | 最大         |

**プロジェクトの選択**: base（精度と速度のバランス）

### 最適化設定

```python
# app/config.py
WHISPER_BEAM_SIZE: 3      # ビームサーチのビーム数（小さいほど高速）
WHISPER_BEST_OF: 3        # 候補数（小さいほど高速）
WHISPER_TEMPERATURE: 0.0  # 決定論的な出力
WHISPER_VAD_ENABLED: True # 音声区間検出（精度向上）
```

## 🔍 トラブルシューティング

### 問題: 30秒を超えると精度が低下する

**原因**:
- 累積バッファが古い音声を削除している
- 文脈が失われている

**解決策**:
- 確定テキストを`initial_prompt`に渡す
- バッファサイズを45-60秒に拡張

### 問題: 幻覚（存在しない内容）が出力される

**原因**:
- 30秒を超える音声を単一セグメントとして処理
- モデルが不安定になっている

**解決策**:
- 適切なチャンキング戦略を使用
- VADで自然な区切りを検出

### 問題: 文字起こしが途中で終わる

**原因**:
- 30秒で切り詰められている
- チャンキングが適切でない

**解決策**:
- faster-whisperの自動チャンキング機能を使用
- セグメント結果を正しく結合

## 📝 まとめ

### 最も重要なポイント

1. **30秒はWhisperモデルのアーキテクチャ上の制約**
   - 変更できない仕様

2. **30秒を超えると幻覚が発生する可能性がある**
   - 存在しない内容が出力される
   - **これが最も重要な問題**

3. **長時間音声は適切なチャンキング戦略が必須**
   - スライディングウィンドウ
   - VADベースのバッチング
   - faster-whisperの自動機能を活用

4. **リアルタイム処理では文脈の保持が重要**
   - 確定テキストを保持
   - `initial_prompt`で文脈を渡す
   - 実際の経過時間を追跡

### 今後の開発での考慮事項

- 新機能開発時は30秒制限を意識する
- 長時間録音機能を追加する場合は、適切なチャンキング戦略を実装する
- メモリ使用量とのトレードオフを考慮する
- ユーザーに対して、適切な使用方法を案内する

## 🔗 関連ドキュメント

- `docs/30_SECOND_ISSUE_INVESTIGATION.md`: 30秒問題の調査・実装方法
- `docs/PHASE4.1_COMPLETION.md`: 累積バッファ方式の実装
- `app/services/cumulative_buffer.py`: 累積バッファの実装
- `app/services/audio_processor.py`: Whisper音声認識の実装
